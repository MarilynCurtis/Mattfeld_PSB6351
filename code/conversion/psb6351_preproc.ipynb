{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will use the following notebook to demonstrate different steps in preprocessing\n",
    "\n",
    "## These steps will include:\n",
    "\n",
    "### 1) Slice timing correction\n",
    "### 2) Motion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Import new things that we'll need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nipype.interfaces.afni as afni\n",
    "import nipype.interfaces.fsl as fsl\n",
    "import nipype.interfaces.freesurfer as fs\n",
    "from nipype.interfaces.utility import Function\n",
    "import seaborn as sns\n",
    "import nibabel as nb\n",
    "import json\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.pipeline.engine as pe \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I next want to get a list of all of my functional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = ['021']\n",
    "base_dir = '/home/mcurt024/Mattfeld_PSB6351'\n",
    "work_dir = '/scratch/classroom/psb6351/mcurt024/try2'\n",
    "func_dir = os.path.join(base_dir, f'dset/sub-{sid[0]}/ses-1/func')\n",
    "fmap_dir = os.path.join(base_dir, f'dset/sub-{sid[0]}/ses-1/fmap')\n",
    "fs_dir = os.path.join(base_dir, 'derivatives', 'freesurfer')\n",
    "\n",
    "# Get a list of my study task json and nifti converted files\n",
    "func_json = sorted(glob(func_dir + '/*.json'))\n",
    "func_files = sorted(glob(func_dir + '/*.nii.gz'))\n",
    "fmap_files = sorted(glob(fmap_dir + '/*func*.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/mcurt024/Mattfeld_PSB6351/dset/sub-021/ses-1/func/sub-021_loc_ROI_run-1_bold.json',\n",
       " '/home/mcurt024/Mattfeld_PSB6351/dset/sub-021/ses-1/func/sub-021_loc_ROI_run-2_bold.json',\n",
       " '/home/mcurt024/Mattfeld_PSB6351/dset/sub-021/ses-1/func/sub-021_task-study_run-1_bold.json',\n",
       " '/home/mcurt024/Mattfeld_PSB6351/dset/sub-021/ses-1/func/sub-021_task-study_run-2_bold.json',\n",
       " '/home/mcurt024/Mattfeld_PSB6351/dset/sub-021/ses-1/func/sub-021_task-study_run-3_bold.json',\n",
       " '/home/mcurt024/Mattfeld_PSB6351/dset/sub-021/ses-1/func/sub-021_task-study_run-4_bold.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next I want to build and run function to perform slice timing correction. I'm going to have to extract some important information from the .json files like the multiband slicetiming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221021-13:45:36,836 nipype.workflow INFO:\n",
      "\t Workflow psb6351_wf settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "221021-13:45:36,957 nipype.workflow INFO:\n",
      "\t Running in parallel.\n",
      "221021-13:45:37,49 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[2] jobs Slots[inf]\n",
      "221021-13:45:37,62 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.id_outliers ID: 0\n",
      "221021-13:45:37,351 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.id_outliers ID: 0\n",
      "221021-13:45:37,367 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.getsubs ID: 1\n",
      "221021-13:45:37,623 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.getsubs ID: 1\n",
      "221021-13:45:45,999 nipype.workflow INFO:\n",
      "\t [Job 1] Completed (psb6351_wf.getsubs).\n",
      "221021-13:45:59,335 nipype.workflow INFO:\n",
      "\t [Job 0] Completed (psb6351_wf.id_outliers).\n",
      "221021-13:45:59,359 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221021-13:45:59,381 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.getbestvol ID: 2\n",
      "221021-13:45:59,641 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.getbestvol ID: 2\n",
      "221021-13:46:03,286 nipype.workflow INFO:\n",
      "\t [Job 2] Completed (psb6351_wf.getbestvol).\n",
      "221021-13:46:03,306 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221021-13:46:03,329 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.extractref ID: 3\n",
      "221021-13:46:03,595 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.extractref ID: 3\n",
      "221021-13:46:17,299 nipype.workflow INFO:\n",
      "\t [Job 3] Completed (psb6351_wf.extractref).\n",
      "221021-13:46:17,324 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[2] jobs Slots[inf]\n",
      "221021-13:46:17,406 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.allineate ID: 5\n",
      "221021-13:46:17,663 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.allineate ID: 5\n",
      "221021-13:46:17,681 nipype.workflow INFO:\n",
      "\t Pending[1] Submitting[6] jobs Slots[inf]\n",
      "221021-13:46:17,702 nipype.workflow INFO:\n",
      "\t Submitting: _volreg0 ID: 9\n",
      "221021-13:46:17,945 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg0 ID: 9\n",
      "221021-13:46:17,964 nipype.workflow INFO:\n",
      "\t Submitting: _volreg1 ID: 10\n",
      "221021-13:46:18,236 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg1 ID: 10\n",
      "221021-13:46:18,254 nipype.workflow INFO:\n",
      "\t Submitting: _volreg2 ID: 11\n",
      "221021-13:46:18,508 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg2 ID: 11\n",
      "221021-13:46:18,525 nipype.workflow INFO:\n",
      "\t Submitting: _volreg3 ID: 12\n",
      "221021-13:46:18,768 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg3 ID: 12\n",
      "221021-13:46:18,784 nipype.workflow INFO:\n",
      "\t Submitting: _volreg4 ID: 13\n",
      "221021-13:46:19,52 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg4 ID: 13\n",
      "221021-13:46:19,70 nipype.workflow INFO:\n",
      "\t Submitting: _volreg5 ID: 14\n",
      "221021-13:46:19,344 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg5 ID: 14\n",
      "221021-13:48:41,949 nipype.workflow INFO:\n",
      "\t [Job 9] Completed (_volreg0).\n",
      "221021-13:49:17,972 nipype.workflow INFO:\n",
      "\t [Job 10] Completed (_volreg1).\n",
      "221021-13:49:44,450 nipype.workflow INFO:\n",
      "\t [Job 11] Completed (_volreg2).\n",
      "221021-13:49:50,477 nipype.workflow INFO:\n",
      "\t [Job 12] Completed (_volreg3).\n",
      "221021-13:49:50,978 nipype.workflow INFO:\n",
      "\t [Job 14] Completed (_volreg5).\n",
      "221021-13:50:18,511 nipype.workflow INFO:\n",
      "\t [Job 13] Completed (_volreg4).\n",
      "221021-13:50:18,534 nipype.workflow INFO:\n",
      "\t Pending[1] Submitting[1] jobs Slots[inf]\n",
      "221021-13:50:18,547 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.volreg ID: 4\n",
      "221021-13:50:18,802 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.volreg ID: 4\n",
      "221021-13:50:22,517 nipype.workflow INFO:\n",
      "\t [Job 4] Completed (psb6351_wf.volreg).\n",
      "221021-13:50:22,537 nipype.workflow INFO:\n",
      "\t Pending[1] Submitting[1] jobs Slots[inf]\n",
      "221021-13:50:22,596 nipype.workflow INFO:\n",
      "\t Pending[1] Submitting[6] jobs Slots[inf]\n",
      "221021-13:50:22,616 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter0 ID: 15\n",
      "221021-13:50:22,880 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter0 ID: 15\n",
      "221021-13:50:22,901 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter1 ID: 16\n",
      "221021-13:50:23,180 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter1 ID: 16\n",
      "221021-13:50:23,201 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter2 ID: 17\n",
      "221021-13:50:23,485 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter2 ID: 17\n",
      "221021-13:50:23,506 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter3 ID: 18\n",
      "221021-13:50:23,782 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter3 ID: 18\n",
      "221021-13:50:23,814 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter4 ID: 19\n",
      "221021-13:50:24,108 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter4 ID: 19\n",
      "221021-13:50:24,119 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter5 ID: 20\n",
      "221021-13:50:24,469 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter5 ID: 20\n",
      "221021-13:50:26,749 nipype.workflow INFO:\n",
      "\t [Job 5] Completed (psb6351_wf.allineate).\n",
      "221021-13:51:44,947 nipype.workflow INFO:\n",
      "\t [Job 16] Completed (_tshifter1).\n",
      "221021-13:51:48,399 nipype.workflow INFO:\n",
      "\t [Job 15] Completed (_tshifter0).\n",
      "221021-13:51:55,95 nipype.workflow INFO:\n",
      "\t [Job 20] Completed (_tshifter5).\n",
      "221021-13:51:56,420 nipype.workflow INFO:\n",
      "\t [Job 17] Completed (_tshifter2).\n",
      "221021-13:51:58,416 nipype.workflow INFO:\n",
      "\t [Job 18] Completed (_tshifter3).\n",
      "221021-13:51:58,654 nipype.workflow INFO:\n",
      "\t [Job 19] Completed (_tshifter4).\n",
      "221021-13:51:58,660 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221021-13:51:58,669 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.tshifter ID: 6\n",
      "221021-13:51:58,907 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.tshifter ID: 6\n",
      "221021-13:52:02,446 nipype.workflow INFO:\n",
      "\t [Job 6] Completed (psb6351_wf.tshifter).\n",
      "221021-13:52:02,451 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221021-13:52:02,497 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[6] jobs Slots[inf]\n",
      "221021-13:52:02,501 nipype.workflow INFO:\n",
      "\t Submitting: _merge0 ID: 21\n",
      "221021-13:52:02,728 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge0 ID: 21\n",
      "221021-13:52:02,732 nipype.workflow INFO:\n",
      "\t Submitting: _merge1 ID: 22\n",
      "221021-13:52:02,986 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge1 ID: 22\n",
      "221021-13:52:02,990 nipype.workflow INFO:\n",
      "\t Submitting: _merge2 ID: 23\n",
      "221021-13:52:03,221 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge2 ID: 23\n",
      "221021-13:52:03,225 nipype.workflow INFO:\n",
      "\t Submitting: _merge3 ID: 24\n",
      "221021-13:52:03,460 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge3 ID: 24\n",
      "221021-13:52:03,463 nipype.workflow INFO:\n",
      "\t Submitting: _merge4 ID: 25\n",
      "221021-13:52:03,707 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge4 ID: 25\n",
      "221021-13:52:03,711 nipype.workflow INFO:\n",
      "\t Submitting: _merge5 ID: 26\n",
      "221021-13:52:04,39 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge5 ID: 26\n",
      "221021-13:52:18,392 nipype.workflow INFO:\n",
      "\t [Job 21] Completed (_merge0).\n",
      "221021-13:52:18,634 nipype.workflow INFO:\n",
      "\t [Job 22] Completed (_merge1).\n",
      "221021-13:52:20,750 nipype.workflow INFO:\n",
      "\t [Job 23] Completed (_merge2).\n",
      "221021-13:52:21,555 nipype.workflow INFO:\n",
      "\t [Job 24] Completed (_merge3).\n",
      "221021-13:52:26,775 nipype.workflow INFO:\n",
      "\t [Job 25] Completed (_merge4).\n",
      "221021-13:52:27,32 nipype.workflow INFO:\n",
      "\t [Job 26] Completed (_merge5).\n",
      "221021-13:52:27,38 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221021-13:52:27,55 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.merge ID: 7\n",
      "221021-13:52:27,304 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.merge ID: 7\n",
      "221021-13:52:32,805 nipype.workflow INFO:\n",
      "\t [Job 7] Completed (psb6351_wf.merge).\n",
      "221021-13:52:32,820 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221021-13:52:32,823 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.datasink ID: 8\n",
      "221021-13:52:33,83 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.datasink ID: 8\n",
      "221021-14:09:53,844 nipype.workflow INFO:\n",
      "\t [Job 8] Completed (psb6351_wf.datasink).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7fcd8e73d310>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I am building a function that eliminates the\n",
    "# mapnode directory structure and assists in saving\n",
    "# all of the outputs into a single directory\n",
    "def get_subs(func_files):\n",
    "    '''Produces Name Substitutions for Each Contrast'''\n",
    "    subs = []\n",
    "    for curr_run in range(len(func_files)):\n",
    "        subs.append(('_tshifter%d' %curr_run, ''))\n",
    "        subs.append(('_volreg%d' %curr_run, ''))\n",
    "    return subs\n",
    "\n",
    "# Here I am building a function that takes in a\n",
    "# text file that includes the number of outliers\n",
    "# at each volume and then finds which volume (e.g., index)\n",
    "# has the minimum number of outliers (e.g., min) \n",
    "# searching over the first 201 volumes\n",
    "# If the index function returns a list because there were\n",
    "# multiple volumes with the same outlier count, pick the first one\n",
    "def best_vol(outlier_count):\n",
    "    best_vol_num = outlier_count.index(min(outlier_count[:200]))\n",
    "    if isinstance(best_vol_num, list):\n",
    "        best_vol_num = best_vol_num[0]\n",
    "    return best_vol_num\n",
    "\n",
    "# Here I am creating a list of lists containing the slice timing for each study run\n",
    "slice_timing_list = []\n",
    "for curr_json in func_json:\n",
    "    curr_json_data = open(curr_json)\n",
    "    curr_func_metadata = json.load(curr_json_data)\n",
    "    slice_timing_list.append(curr_func_metadata['SliceTiming'])\n",
    "\n",
    "# Here I am establishing a nipype work flow that I will eventually execute\n",
    "psb6351_wf = pe.Workflow(name='psb6351_wf')\n",
    "psb6351_wf.base_dir = work_dir + f'/psb6351workdir/sub-{sid[0]}'\n",
    "psb6351_wf.config['execution']['use_relative_paths'] = True\n",
    "\n",
    "# Create a Function node to substitute names of files created during pipeline\n",
    "getsubs = pe.Node(Function(input_names=['func_files'],\n",
    "                           output_names=['subs'],\n",
    "                           function=get_subs),\n",
    "                  name='getsubs')\n",
    "getsubs.inputs.func_files = func_files\n",
    "\n",
    "# Here I am inputing just the first run functional data\n",
    "# I want to use afni's 3dToutcount to find the number of \n",
    "# outliers at each volume.  I will use this information to\n",
    "# later select the earliest volume with the least number of outliers\n",
    "# to serve as the base for the motion correction\n",
    "id_outliers = pe.Node(afni.OutlierCount(),\n",
    "                      name = 'id_outliers')\n",
    "id_outliers.inputs.in_file = func_files[0]\n",
    "id_outliers.inputs.automask = True\n",
    "id_outliers.inputs.out_file = 'outlier_file'\n",
    "\n",
    "# Create a Function node to identify the best volume based\n",
    "# on the number of outliers at each volume. I'm searching\n",
    "# for the index in the first 201 volumes that has the \n",
    "# minimum number of outliers and will use the min() function\n",
    "# I will use the index function to get the best vol. \n",
    "getbestvol = pe.Node(Function(input_names=['outlier_count'],\n",
    "                              output_names=['best_vol_num'],\n",
    "                              function=best_vol),\n",
    "                     name='getbestvol')\n",
    "psb6351_wf.connect(id_outliers, 'out_file', getbestvol, 'outlier_count')\n",
    "\n",
    "# Extract the earliest volume with the\n",
    "# the fewest outliers of the first run as the reference \n",
    "extractref = pe.Node(fsl.ExtractROI(t_size=1),\n",
    "                     name = \"extractref\")\n",
    "extractref.inputs.in_file = func_files[0]\n",
    "#extractref.inputs.t_min = int(np.ceil(nb.load(study_func_files[0]).shape[3]/2)) #PICKING MIDDLE\n",
    "psb6351_wf.connect(getbestvol, 'best_vol_num', extractref, 't_min')\n",
    "\n",
    "\n",
    "# Below is the command that runs AFNI's 3dvolreg command.\n",
    "# this is the node that performs the motion correction\n",
    "# I'm iterating over the functional files which I am passing\n",
    "# functional data from the slice timing correction node before\n",
    "# I'm using the earliest volume with the least number of outliers\n",
    "# during the first run as the base file to register to.\n",
    "\n",
    "volreg = pe.MapNode(afni.Volreg(),\n",
    "                    iterfield=['in_file'],\n",
    "                    name = 'volreg')\n",
    "volreg.inputs.outputtype = 'NIFTI_GZ'\n",
    "volreg.inputs.in_file = func_files\n",
    "psb6351_wf.connect(extractref, 'roi_file', volreg, 'basefile')\n",
    "\n",
    "# Below is the command that runs AFNI's 3dTshift command\n",
    "# this is the node that performs the slice timing correction\n",
    "# I input the study func files as a list and the slice timing \n",
    "# as a list of lists. I'm using a MapNode to iterate over the two.\n",
    "# this should allow me to parallelize this on the HPC\n",
    "\n",
    "tshifter = pe.MapNode(afni.TShift(),\n",
    "                      iterfield=['in_file','slice_timing'],\n",
    "                      name = 'tshifter')\n",
    "tshifter.inputs.tr = '1.76'\n",
    "tshifter.inputs.slice_timing = slice_timing_list\n",
    "tshifter.inputs.outputtype = 'NIFTI_GZ'\n",
    "psb6351_wf.connect(volreg, 'out_file', tshifter, 'in_file')\n",
    "\n",
    "#Below is the coregistration step from homework 3. With this, we\n",
    "#can overlay the functional and structural scans.\n",
    "\n",
    "allineate = pe.Node(afni.Allineate(), name = 'allineate')\n",
    "allineate.inputs.reference = f'{base_dir}/dset/sub-{sid[0]}/ses-1/anat/sub-021_run-2_T1w.nii.gz'\n",
    "psb6351_wf.connect(extractref, 'roi_file', allineate, 'in_file')\n",
    "\n",
    "\n",
    "#Below is the blurring step from homework 4. \n",
    "\n",
    "merge = pe.MapNode(afni.Merge(), iterfield = ['blurfwhm', 'in_files'], name = 'merge')\n",
    "merge.inputs.blurfwhm = [3,6,9,12,14,16]\n",
    "merge.inputs.doall = True\n",
    "psb6351_wf.connect(tshifter, 'out_file', merge, 'in_files')\n",
    "\n",
    "\n",
    "# Below is the node that collects all the data and saves\n",
    "# the outputs that I am interested in. Here in this node\n",
    "# I use the substitutions input combined with the earlier\n",
    "# function to get rid of nesting\n",
    "datasink = pe.Node(nio.DataSink(), name=\"datasink\")\n",
    "datasink.inputs.base_directory = os.path.join(base_dir, 'derivatives/preproc')\n",
    "datasink.inputs.container = f'sub-{sid[0]}'\n",
    "psb6351_wf.connect(tshifter, 'out_file', datasink, 'sltime_corr')\n",
    "psb6351_wf.connect(extractref, 'roi_file', datasink, 'study_ref')\n",
    "psb6351_wf.connect(volreg, 'out_file', datasink, 'motion.@corrfile')\n",
    "psb6351_wf.connect(volreg, 'oned_matrix_save', datasink, 'motion.@matrix')\n",
    "psb6351_wf.connect(volreg, 'oned_file', datasink, 'motion.@par')\n",
    "psb6351_wf.connect(getsubs, 'subs', datasink, 'substitutions')\n",
    "psb6351_wf.connect(allineate, 'out_matrix', datasink, 'reg')\n",
    "psb6351_wf.connect(merge, 'out_file', datasink, 'blur')\n",
    "\n",
    "# The following two lines set a work directory outside of my \n",
    "# local git repo and runs the workflow\n",
    "psb6351_wf.run(plugin='SLURM',\n",
    "               plugin_args={'sbatch_args': ('--partition classroom --qos pq_psb6351 --account acc_psb6351'),\n",
    "                            'overwrite':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From HW2: this code switches the order of tshifter and volreg\n",
    "\n",
    "\n",
    "#tshifter = pe.MapNode(afni.TShift(),\n",
    " #                     iterfield=['in_file','slice_timing'],\n",
    "  #                    name = 'tshifter')\n",
    "#tshifter.inputs.tr = '1.76'\n",
    "#tshifter.inputs.slice_timing = slice_timing_list\n",
    "#tshifter.inputs.outputtype = 'NIFTI_GZ'\n",
    "#tshifter.inputs.in_file = func_files\n",
    "\n",
    "\n",
    "#volreg = pe.MapNode(afni.Volreg(),\n",
    " #                   iterfield=['in_file'],\n",
    "  #                  name = 'volreg')\n",
    "#volreg.inputs.outputtype = 'NIFTI_GZ'\n",
    "#psb6351_wf.connect(tshifter, 'out_file', volreg, 'in_file')\n",
    "#psb6351_wf.connect(extractref, 'roi_file', volreg, 'basefile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I will load and plot the motion files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_dir = os.path.join(base_dir, f'derivatives/preproc/sub-{sid[0]}/motion')\n",
    "study_motion_files = sorted(glob(motion_dir + '/*study*_bold_tshift.1D'))\n",
    "\n",
    "for curr_mot_file in study_motion_files:\n",
    "    motion_df = pd.read_csv(curr_mot_file, sep=\"  \", header=None)\n",
    "    motion_df.columns = ['roll', 'pitch', 'yaw', 'dS', 'dL', 'dP']\n",
    "\n",
    "    num_vols = range(1, len(motion_df)+1)\n",
    "    fig, axs = plt.subplots(motion_df.shape[1], 1, figsize = (15, 10))\n",
    "    # make a little extra space between the subplots\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    for idx, curr_col in enumerate(motion_df.keys()):\n",
    "        axs[idx].plot(num_vols, motion_df[f'{curr_col}'])\n",
    "        axs[idx].set_xlabel('TRs')\n",
    "        axs[idx].set_ylabel(f'{curr_col}')\n",
    "        axs[idx].grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.0\n",
      "135.0\n"
     ]
    }
   ],
   "source": [
    "study_motcorr_files = sorted(glob(motion_dir + '/*.nii.gz'))\n",
    "study_motcorr_img_data = nb.load(study_motcorr_files[0]).get_fdata()\n",
    "study_orig_img_data = nb.load(func_files[0]).get_fdata()\n",
    "\n",
    "#test_motcorr_img_data.shape\n",
    "\n",
    "print(study_motcorr_img_data[50,50,32,50])\n",
    "print(study_orig_img_data[50,50,32,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_file = glob(base_dir + f'/derivatives/preproc/sub-{sid[0]}/reg/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reg_matrix \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_table(reg_file[\u001b[38;5;241m1\u001b[39m], header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(reg_matrix)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "reg_matrix = pd.read_table(reg_file[1], header=None)\n",
    "print(reg_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
