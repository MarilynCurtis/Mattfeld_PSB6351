{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will use the following notebook to demonstrate different steps in preprocessing\n",
    "\n",
    "## These steps will include:\n",
    "\n",
    "### 1) Slice timing correction\n",
    "### 2) Motion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Import new things that we'll need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nipype.interfaces.afni as afni\n",
    "import nipype.interfaces.fsl as fsl\n",
    "import nipype.interfaces.freesurfer as fs\n",
    "from nipype.interfaces.utility import Function\n",
    "import seaborn as sns\n",
    "import nibabel as nb\n",
    "import json\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.pipeline.engine as pe \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I next want to get a list of all of my functional files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cells will run the motion correction first then the slice timing correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = ['021']\n",
    "base_dir = '/home/mcurt024/Mattfeld_PSB6351'\n",
    "work_dir = '/scratch/classroom/psb6351/mcurt024/try2'\n",
    "func_dir = os.path.join(base_dir, f'dset/sub-{sid[0]}/ses-1/func')\n",
    "fmap_dir = os.path.join(base_dir, f'dset/sub-{sid[0]}/ses-1/fmap')\n",
    "fs_dir = os.path.join(base_dir, 'derivatives', 'freesurfer')\n",
    "\n",
    "# Get a list of my study task json and nifti converted files\n",
    "func_json = sorted(glob(func_dir + '/*.json'))\n",
    "func_files = sorted(glob(func_dir + '/*.nii.gz'))\n",
    "fmap_files = sorted(glob(fmap_dir + '/*func*.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next I want to build and run function to perform slice timing correction. I'm going to have to extract some important information from the .json files like the multiband slicetiming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221209-17:50:58,374 nipype.workflow INFO:\n",
      "\t Workflow psb6351_wf settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "221209-17:50:58,767 nipype.workflow INFO:\n",
      "\t Running in parallel.\n",
      "221209-17:50:58,977 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[2] jobs Slots[inf]\n",
      "221209-17:50:58,994 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.id_outliers ID: 0\n",
      "221209-17:50:59,40 nipype.workflow INFO:\n",
      "\t [Job 0] Cached (psb6351_wf.id_outliers).\n",
      "221209-17:50:59,48 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.id_outliers ID: 0\n",
      "221209-17:50:59,50 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.getsubs ID: 1\n",
      "221209-17:50:59,56 nipype.workflow INFO:\n",
      "\t [Job 1] Cached (psb6351_wf.getsubs).\n",
      "221209-17:50:59,59 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.getsubs ID: 1\n",
      "221209-17:50:59,62 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221209-17:50:59,64 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.getbestvol ID: 2\n",
      "221209-17:50:59,268 nipype.workflow INFO:\n",
      "\t [Job 2] Cached (psb6351_wf.getbestvol).\n",
      "221209-17:50:59,273 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.getbestvol ID: 2\n",
      "221209-17:50:59,275 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221209-17:50:59,278 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.extractref ID: 3\n",
      "221209-17:50:59,290 nipype.workflow INFO:\n",
      "\t [Job 3] Cached (psb6351_wf.extractref).\n",
      "221209-17:50:59,292 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.extractref ID: 3\n",
      "221209-17:50:59,296 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[2] jobs Slots[inf]\n",
      "221209-17:50:59,373 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.afni_register ID: 5\n",
      "221209-17:50:59,386 nipype.workflow INFO:\n",
      "\t [Job 5] Cached (psb6351_wf.afni_register).\n",
      "221209-17:50:59,390 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.afni_register ID: 5\n",
      "221209-17:50:59,404 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[6] jobs Slots[inf]\n",
      "221209-17:50:59,407 nipype.workflow INFO:\n",
      "\t Submitting: _volreg0 ID: 9\n",
      "221209-17:50:59,411 nipype.workflow INFO:\n",
      "\t [Job 9] Cached (_volreg0).\n",
      "221209-17:50:59,413 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg0 ID: 9\n",
      "221209-17:50:59,415 nipype.workflow INFO:\n",
      "\t Submitting: _volreg1 ID: 10\n",
      "221209-17:50:59,438 nipype.workflow INFO:\n",
      "\t [Job 10] Cached (_volreg1).\n",
      "221209-17:50:59,441 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg1 ID: 10\n",
      "221209-17:50:59,443 nipype.workflow INFO:\n",
      "\t Submitting: _volreg2 ID: 11\n",
      "221209-17:50:59,448 nipype.workflow INFO:\n",
      "\t [Job 11] Cached (_volreg2).\n",
      "221209-17:50:59,450 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg2 ID: 11\n",
      "221209-17:50:59,452 nipype.workflow INFO:\n",
      "\t Submitting: _volreg3 ID: 12\n",
      "221209-17:50:59,458 nipype.workflow INFO:\n",
      "\t [Job 12] Cached (_volreg3).\n",
      "221209-17:50:59,460 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg3 ID: 12\n",
      "221209-17:50:59,462 nipype.workflow INFO:\n",
      "\t Submitting: _volreg4 ID: 13\n",
      "221209-17:50:59,466 nipype.workflow INFO:\n",
      "\t [Job 13] Cached (_volreg4).\n",
      "221209-17:50:59,468 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg4 ID: 13\n",
      "221209-17:50:59,470 nipype.workflow INFO:\n",
      "\t Submitting: _volreg5 ID: 14\n",
      "221209-17:50:59,477 nipype.workflow INFO:\n",
      "\t [Job 14] Cached (_volreg5).\n",
      "221209-17:50:59,479 nipype.workflow INFO:\n",
      "\t Finished submitting: _volreg5 ID: 14\n",
      "221209-17:50:59,482 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221209-17:50:59,486 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.volreg ID: 4\n",
      "221209-17:50:59,507 nipype.workflow INFO:\n",
      "\t [Job 4] Cached (psb6351_wf.volreg).\n",
      "221209-17:50:59,518 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.volreg ID: 4\n",
      "221209-17:50:59,524 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221209-17:50:59,593 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[6] jobs Slots[inf]\n",
      "221209-17:50:59,601 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter0 ID: 15\n",
      "221209-17:50:59,613 nipype.workflow INFO:\n",
      "\t [Job 15] Cached (_tshifter0).\n",
      "221209-17:50:59,616 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter0 ID: 15\n",
      "221209-17:50:59,618 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter1 ID: 16\n",
      "221209-17:50:59,625 nipype.workflow INFO:\n",
      "\t [Job 16] Cached (_tshifter1).\n",
      "221209-17:50:59,627 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter1 ID: 16\n",
      "221209-17:50:59,629 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter2 ID: 17\n",
      "221209-17:50:59,633 nipype.workflow INFO:\n",
      "\t [Job 17] Cached (_tshifter2).\n",
      "221209-17:50:59,659 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter2 ID: 17\n",
      "221209-17:50:59,661 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter3 ID: 18\n",
      "221209-17:50:59,675 nipype.workflow INFO:\n",
      "\t [Job 18] Cached (_tshifter3).\n",
      "221209-17:50:59,678 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter3 ID: 18\n",
      "221209-17:50:59,679 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter4 ID: 19\n",
      "221209-17:50:59,685 nipype.workflow INFO:\n",
      "\t [Job 19] Cached (_tshifter4).\n",
      "221209-17:50:59,689 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter4 ID: 19\n",
      "221209-17:50:59,692 nipype.workflow INFO:\n",
      "\t Submitting: _tshifter5 ID: 20\n",
      "221209-17:50:59,705 nipype.workflow INFO:\n",
      "\t [Job 20] Cached (_tshifter5).\n",
      "221209-17:50:59,707 nipype.workflow INFO:\n",
      "\t Finished submitting: _tshifter5 ID: 20\n",
      "221209-17:50:59,710 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221209-17:50:59,716 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.tshifter ID: 6\n",
      "221209-17:50:59,745 nipype.workflow INFO:\n",
      "\t [Job 6] Cached (psb6351_wf.tshifter).\n",
      "221209-17:50:59,757 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.tshifter ID: 6\n",
      "221209-17:50:59,764 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221209-17:50:59,837 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[6] jobs Slots[inf]\n",
      "221209-17:50:59,856 nipype.workflow INFO:\n",
      "\t Submitting: _merge0 ID: 21\n",
      "221209-17:50:59,863 nipype.workflow INFO:\n",
      "\t [Job 21] Cached (_merge0).\n",
      "221209-17:50:59,865 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge0 ID: 21\n",
      "221209-17:50:59,867 nipype.workflow INFO:\n",
      "\t Submitting: _merge1 ID: 22\n",
      "221209-17:50:59,872 nipype.workflow INFO:\n",
      "\t [Job 22] Cached (_merge1).\n",
      "221209-17:50:59,874 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge1 ID: 22\n",
      "221209-17:50:59,877 nipype.workflow INFO:\n",
      "\t Submitting: _merge2 ID: 23\n",
      "221209-17:50:59,892 nipype.workflow INFO:\n",
      "\t [Job 23] Cached (_merge2).\n",
      "221209-17:50:59,894 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge2 ID: 23\n",
      "221209-17:50:59,896 nipype.workflow INFO:\n",
      "\t Submitting: _merge3 ID: 24\n",
      "221209-17:50:59,911 nipype.workflow INFO:\n",
      "\t [Job 24] Cached (_merge3).\n",
      "221209-17:50:59,914 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge3 ID: 24\n",
      "221209-17:50:59,918 nipype.workflow INFO:\n",
      "\t Submitting: _merge4 ID: 25\n",
      "221209-17:50:59,926 nipype.workflow INFO:\n",
      "\t [Job 25] Cached (_merge4).\n",
      "221209-17:50:59,928 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge4 ID: 25\n",
      "221209-17:50:59,929 nipype.workflow INFO:\n",
      "\t Submitting: _merge5 ID: 26\n",
      "221209-17:50:59,940 nipype.workflow INFO:\n",
      "\t [Job 26] Cached (_merge5).\n",
      "221209-17:50:59,942 nipype.workflow INFO:\n",
      "\t Finished submitting: _merge5 ID: 26\n",
      "221209-17:50:59,945 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221209-17:50:59,950 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.merge ID: 7\n",
      "221209-17:50:59,962 nipype.workflow INFO:\n",
      "\t [Node] Outdated cache found for \"psb6351_wf.merge\".\n",
      "221209-17:51:02,551 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.merge ID: 7\n",
      "221209-17:51:27,82 nipype.workflow INFO:\n",
      "\t [Job 7] Completed (psb6351_wf.merge).\n",
      "221209-17:51:27,111 nipype.workflow INFO:\n",
      "\t Pending[0] Submitting[1] jobs Slots[inf]\n",
      "221209-17:51:27,113 nipype.workflow INFO:\n",
      "\t Submitting: psb6351_wf.datasink ID: 8\n",
      "221209-17:51:27,625 nipype.workflow INFO:\n",
      "\t Finished submitting: psb6351_wf.datasink ID: 8\n",
      "221209-17:52:29,137 nipype.workflow INFO:\n",
      "\t [Job 8] Completed (psb6351_wf.datasink).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7f14dcb5e340>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I am building a function that eliminates the\n",
    "# mapnode directory structure and assists in saving\n",
    "# all of the outputs into a single directory\n",
    "def get_subs(func_files):\n",
    "    '''Produces Name Substitutions for Each Contrast'''\n",
    "    subs = []\n",
    "    for curr_run in range(len(func_files)):\n",
    "        subs.append(('_tshifter%d' %curr_run, ''))\n",
    "        subs.append(('_volreg%d' %curr_run, ''))\n",
    "    return subs\n",
    "\n",
    "# Here I am building a function that takes in a\n",
    "# text file that includes the number of outliers\n",
    "# at each volume and then finds which volume (e.g., index)\n",
    "# has the minimum number of outliers (e.g., min) \n",
    "# searching over the first 201 volumes\n",
    "# If the index function returns a list because there were\n",
    "# multiple volumes with the same outlier count, pick the first one\n",
    "def best_vol(outlier_count):\n",
    "    best_vol_num = outlier_count.index(min(outlier_count[:200]))\n",
    "    if isinstance(best_vol_num, list):\n",
    "        best_vol_num = best_vol_num[0]\n",
    "    return best_vol_num\n",
    "\n",
    "# Here I am creating a list of lists containing the slice timing for each study run\n",
    "slice_timing_list = []\n",
    "for curr_json in func_json:\n",
    "    curr_json_data = open(curr_json)\n",
    "    curr_func_metadata = json.load(curr_json_data)\n",
    "    slice_timing_list.append(curr_func_metadata['SliceTiming'])\n",
    "\n",
    "# Here I am establishing a nipype work flow that I will eventually execute\n",
    "psb6351_wf = pe.Workflow(name='psb6351_wf')\n",
    "psb6351_wf.base_dir = work_dir + f'/psb6351workdir/sub-{sid[0]}'\n",
    "psb6351_wf.config['execution']['use_relative_paths'] = True\n",
    "\n",
    "# Create a Function node to substitute names of files created during pipeline\n",
    "getsubs = pe.Node(Function(input_names=['func_files'],\n",
    "                           output_names=['subs'],\n",
    "                           function=get_subs),\n",
    "                  name='getsubs')\n",
    "getsubs.inputs.func_files = func_files\n",
    "\n",
    "# Here I am inputing just the first run functional data\n",
    "# I want to use afni's 3dToutcount to find the number of \n",
    "# outliers at each volume.  I will use this information to\n",
    "# later select the earliest volume with the least number of outliers\n",
    "# to serve as the base for the motion correction\n",
    "id_outliers = pe.Node(afni.OutlierCount(),\n",
    "                      name = 'id_outliers')\n",
    "id_outliers.inputs.in_file = func_files[0]\n",
    "id_outliers.inputs.automask = True\n",
    "id_outliers.inputs.out_file = 'outlier_file'\n",
    "\n",
    "# Create a Function node to identify the best volume based\n",
    "# on the number of outliers at each volume. I'm searching\n",
    "# for the index in the first 201 volumes that has the \n",
    "# minimum number of outliers and will use the min() function\n",
    "# I will use the index function to get the best vol. \n",
    "getbestvol = pe.Node(Function(input_names=['outlier_count'],\n",
    "                              output_names=['best_vol_num'],\n",
    "                              function=best_vol),\n",
    "                     name='getbestvol')\n",
    "psb6351_wf.connect(id_outliers, 'out_file', getbestvol, 'outlier_count')\n",
    "\n",
    "# Extract the earliest volume with the\n",
    "# the fewest outliers of the first run as the reference \n",
    "extractref = pe.Node(fsl.ExtractROI(t_size=1),\n",
    "                     name = \"extractref\")\n",
    "extractref.inputs.in_file = func_files[0]\n",
    "#extractref.inputs.t_min = int(np.ceil(nb.load(study_func_files[0]).shape[3]/2)) #PICKING MIDDLE\n",
    "psb6351_wf.connect(getbestvol, 'best_vol_num', extractref, 't_min')\n",
    "\n",
    "\n",
    "# Below is the command that runs AFNI's 3dvolreg command.\n",
    "# this is the node that performs the motion correction\n",
    "# I'm iterating over the functional files which I am passing\n",
    "# functional data from the slice timing correction node before\n",
    "# I'm using the earliest volume with the least number of outliers\n",
    "# during the first run as the base file to register to.\n",
    "\n",
    "volreg = pe.MapNode(afni.Volreg(),\n",
    "                    iterfield=['in_file'],\n",
    "                    name = 'volreg')\n",
    "volreg.inputs.outputtype = 'NIFTI_GZ'\n",
    "volreg.inputs.in_file = func_files\n",
    "psb6351_wf.connect(extractref, 'roi_file', volreg, 'basefile')\n",
    "\n",
    "# Below is the command that runs AFNI's 3dTshift command\n",
    "# this is the node that performs the slice timing correction\n",
    "# I input the study func files as a list and the slice timing \n",
    "# as a list of lists. I'm using a MapNode to iterate over the two.\n",
    "# this should allow me to parallelize this on the HPC\n",
    "\n",
    "tshifter = pe.MapNode(afni.TShift(),\n",
    "                      iterfield=['in_file','slice_timing'],\n",
    "                      name = 'tshifter')\n",
    "tshifter.inputs.tr = '1.76'\n",
    "tshifter.inputs.slice_timing = slice_timing_list\n",
    "tshifter.inputs.outputtype = 'NIFTI_GZ'\n",
    "psb6351_wf.connect(volreg, 'out_file', tshifter, 'in_file')\n",
    "\n",
    "#Below is the coregistration step from homework 3. With this, we\n",
    "#can overlay the functional and structural scans, thereby aligning EPI and anatomical datasets.\n",
    "\n",
    "\n",
    "afni_register = pe.Node(afni.AlignEpiAnatPy(), name = 'afni_register')\n",
    "afni_register.inputs.anat = f'{base_dir}/dset/sub-{sid[0]}/ses-1/anat/sub-021_run-2_T1w.nii.gz'\n",
    "afni_register.inputs.epi_base = 0\n",
    "afni_register.inputs.anat2epi = True\n",
    "afni_register.inputs.outputtype = 'NIFTI_GZ'\n",
    "psb6351_wf.connect(extractref, 'roi_file', afni_register, 'in_file')\n",
    "\n",
    "\n",
    "#Below is the smoothing step from homework 4. \n",
    "\n",
    "merge = pe.MapNode(afni.Merge(), iterfield = ['in_files'], name = 'merge')\n",
    "merge.inputs.blurfwhm = 3\n",
    "merge.inputs.doall = True\n",
    "psb6351_wf.connect(tshifter, 'out_file', merge, 'in_files')\n",
    "\n",
    "\n",
    "# Below is the node that collects all the data and saves\n",
    "# the outputs that I am interested in. Here in this node\n",
    "# I use the substitutions input combined with the earlier\n",
    "# function to get rid of nesting\n",
    "\n",
    "datasink = pe.Node(nio.DataSink(), name=\"datasink\")\n",
    "datasink.inputs.base_directory = os.path.join(base_dir, 'derivatives/preproc')\n",
    "datasink.inputs.container = f'sub-{sid[0]}'\n",
    "psb6351_wf.connect(tshifter, 'out_file', datasink, 'sltime_corr')\n",
    "psb6351_wf.connect(extractref, 'roi_file', datasink, 'study_ref')\n",
    "psb6351_wf.connect(volreg, 'out_file', datasink, 'motion.@corrfile')\n",
    "psb6351_wf.connect(volreg, 'oned_matrix_save', datasink, 'motion.@matrix')\n",
    "psb6351_wf.connect(volreg, 'oned_file', datasink, 'motion.@par')\n",
    "psb6351_wf.connect(getsubs, 'subs', datasink, 'substitutions')\n",
    "psb6351_wf.connect(afni_register, 'anat_al_mat', datasink, 'afniregister')\n",
    "psb6351_wf.connect(merge, 'out_file', datasink, 'blur')\n",
    "\n",
    "# The following two lines set a work directory outside of my \n",
    "# local git repo and runs the workflow\n",
    "psb6351_wf.run(plugin='SLURM',\n",
    "               plugin_args={'sbatch_args': ('--partition classroom --qos pq_psb6351 --account acc_psb6351'),\n",
    "                            'overwrite':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I will load and plot the motion files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_dir = os.path.join(base_dir, f'derivatives/preproc/sub-{sid[0]}/motion')\n",
    "study_motion_files = sorted(glob(motion_dir + '/*study*_bold_tshift.1D'))\n",
    "\n",
    "for curr_mot_file in study_motion_files:\n",
    "    motion_df = pd.read_csv(curr_mot_file, sep=\"  \", header=None)\n",
    "    motion_df.columns = ['roll', 'pitch', 'yaw', 'dS', 'dL', 'dP']\n",
    "\n",
    "    num_vols = range(1, len(motion_df)+1)\n",
    "    fig, axs = plt.subplots(motion_df.shape[1], 1, figsize = (15, 10))\n",
    "    # make a little extra space between the subplots\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    for idx, curr_col in enumerate(motion_df.keys()):\n",
    "        axs[idx].plot(num_vols, motion_df[f'{curr_col}'])\n",
    "        axs[idx].set_xlabel('TRs')\n",
    "        axs[idx].set_ylabel(f'{curr_col}')\n",
    "        axs[idx].grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.0\n",
      "135.0\n"
     ]
    }
   ],
   "source": [
    "study_motcorr_files = sorted(glob(motion_dir + '/*.nii.gz'))\n",
    "study_motcorr_img_data = nb.load(study_motcorr_files[0]).get_fdata()\n",
    "study_orig_img_data = nb.load(func_files[0]).get_fdata()\n",
    "\n",
    "print(study_motcorr_img_data[50,50,32,50])\n",
    "print(study_orig_img_data[50,50,32,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_file = glob(base_dir + f'/derivatives/preproc/sub-{sid[0]}/afniregister/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0        1.00388   -0.00982463    0.00352163     ...\n"
     ]
    }
   ],
   "source": [
    "reg_matrix = pd.read_table(reg_file[0], header=None)\n",
    "print(reg_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
